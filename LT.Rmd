---
title: "パッケージを作った(仮)"
subtitle: "Tokyo.R #59 LT"
author: "@y__mattu"
date: "2017/3/18"
output:
  revealjs::revealjs_presentation:
    transition: convex
    css: for_revealjs.css
    theme: sky
    highlight: kate
    center: true
    self_contained: false
    reveal_plugins: ["chalkboard"]
    reveal_options:
      slideNumber: true
      chalkboard:
        theme: whiteboard
---
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# はじめに

## 自己紹介

<div class="column1">
- 松村優哉(@y__mattu)
- 大学 4 年生
- 計量経済学、ベイズ統計、因果推論、マーケティング
- 言語: R, SAS, Python
- 近況: もうすぐ卒業&進学
</div>

<div class="column2">
![icon](./img/twitter_icon.jpg)
</div>

# パッケージを作った

## 何のパッケージか
- MlBayesOpt(https://github.com/ymattu/MlBayesOpt)
- 機械学習のベイズ最適化によるパラメータチューニングを楽に書くパッケージ
- SVM(RBF カーネル)、ランダムフォレスト、XGboost に対応
- ベースパッケージは SVM(e1071), RF(ranger), XGboost(xgboost)
- 精度評価はホールド・アウト検証

## 背景
- ベイズ最適化のパッケージはもともとあった({rBayesianOptimization})
- 機械学習のパラメータチューニングもできた
- でも少し大変だった

## これまでのベイズ最適化(例: XGboost)
```{r eval = FALSE}
library(xgboost)
library(Matrix)
library(rBayesianOptimization)

odd.n <- 2*(1:75)-1
iris_train <- iris[odd.n, ] # 奇数を訓練データ
iris_test <- iris[-odd.n, ] # 偶数を検証データ
```

## これまでのベイズ最適化(例: XGboost)
```{r eval=FALSE}
# データセットを xgboost で扱える形式に直す
train.mx <- sparse.model.matrix(Species ~., iris_train)
test.mx <- sparse.model.matrix(Species ~ ., iris_test)
dtrain <- xgb.DMatrix(train.mx, label = as.integer(iris_train$Species) - 1)
dtest <- xgb.DMatrix(test.mx, label = as.integer(iris_test$Species) - 1)

# 最大化する関数の作成
xgb_holdout <- function(ex, mx, nr){
    model <- xgb.train(params=list(objective = "multi:softmax", num_class = 10, eval_metric = "mlogloss",
                                   eta = ex, max_depth = mx),
                                   data = dtrain, nrounds = nr)
    t.pred <- predict(model, newdata = dtest)
    Pred <- sum(diag(table(test$label, t.pred)))/nrow(test)
    list(Score = Pred, Pred = Pred)
}
```

## これまでのベイズ最適化(例: XGboost)
```{r eval=FALSE}
# ベイズ最適化
opt_xgb <- BayesianOptimization(xgb_holdout,
                                bounds=list(ex = c(2L,5L), mx = c(4L,5L), nr = c(70L,160L)),
                                init_points = 20, n_iter = 1, acq = 'ei', kappa = 2.576,
                                eps = 0.0, verbose = TRUE)

```

長い! もっと短く!

# {MlBayesOpt}

## インストールと読み込み
### インストール
```{r eval=FALSE}
devtools::install_github("ymattu/MlBayesOpt")
```

### 読み込み
```{r eval=FALSE}
library(MlBayesOpt)
```

# 使い方
## SVM
```{r eval=FALSE}
set.seed(123)

res <- svm_opt(
  train_data = iris_train,
  train_label = iris_train$Species,
  test_data = iris_test,
  test_label = iris_test$Species,
  acq = "ucb"
  )
```

## SVM の最適化出力(抜粋)
```{r eval=FALSE}
elapsed = 0.00	Round = 1	gamma_opt = 6.e+04	cost_opt = 42.9050	Value = 0.3333
elapsed = 0.01	Round = 2	gamma_opt = 6.e+04	cost_opt = 12.0327	Value = 0.3333
elapsed = 0.00	Round = 3	gamma_opt = 7.e+04	cost_opt = 92.1573	Value = 0.3333
elapsed = 0.01	Round = 4	gamma_opt = 9.e+04	cost_opt = 18.3716	Value = 0.3333
elapsed = 0.01	Round = 5	gamma_opt = 8.e+04	cost_opt = 56.2588	Value = 0.3333
# 中略
elapsed = 0.00	Round = 19	gamma_opt = 2453.1625	cost_opt = 84.8863	Value = 0.3733
elapsed = 0.00	Round = 20	gamma_opt = 1.e+05	cost_opt = 62.2435	Value = 0.3333
elapsed = 0.01	Round = 21	gamma_opt = 1.e+04	cost_opt = 23.6688	Value = 0.5867

 Best Parameters Found:
Round = 15	gamma_opt = 1.e+04	cost_opt = 79.5983	Value = 0.6133
```


## ランダムフォレスト
```{r eval=FALSE}
set.seed(123)

mod <- rf_opt(
  train_data = iris_train,
  train_label = iris_train$Species,
  test_data = iris_test,
  test_label = iris_test$Species,
  mtry_range = c(1L, 4L)
  )
```


## ランダムフォレストの最適化出力(抜粋)
```{r eval=FALSE}
elapsed = 0.01	Round = 1	num_trees_opt = 288.0000	mtry_opt = 4.0000	Value = 1.0000
elapsed = 0.03	Round = 2	num_trees_opt = 789.0000	mtry_opt = 3.0000	Value = 1.0000
elapsed = 0.02	Round = 3	num_trees_opt = 410.0000	mtry_opt = 3.0000	Value = 1.0000
elapsed = 0.04	Round = 4	num_trees_opt = 883.0000	mtry_opt = 4.0000	Value = 1.0000
elapsed = 0.03	Round = 5	num_trees_opt = 941.0000	mtry_opt = 3.0000	Value = 1.0000
# 中略
elapsed = 0.01	Round = 19	num_trees_opt = 329.0000	mtry_opt = 2.0000	Value = 1.0000
elapsed = 0.03	Round = 20	num_trees_opt = 955.0000	mtry_opt = 2.0000	Value = 1.0000
elapsed = 0.01	Round = 21	num_trees_opt = 101.0000	mtry_opt = 2.0000	Value = 1.0000

 Best Parameters Found:
Round = 1	num_trees_opt = 288.0000	mtry_opt = 4.0000	Value = 1.0000
```

## XGboost
```{r eval=FALSE}
set.seed(71)

res1 <- xgb_opt(train_data = iris_train,
               train_label = iris_train$Species,
               test_data = iris_test,
               test_label = iris_test$Species,
               objectfun = "multi:softmax",
               classes = 3,
               evalmetric = "merror"
)
```


## XGboost の最適化出力(抜粋)
```{r eval=FALSE}
elapsed = 0.02	Round = 1	eta_opt = 0.8729	max_depth_opt = 6.0000	nrounds_opt = 123.8761	subsample_opt = 0.2789	bytree_opt = 0.5343	Value = 0.7467
elapsed = 0.02	Round = 2	eta_opt = 0.5779	max_depth_opt = 6.0000	nrounds_opt = 144.4570	subsample_opt = 0.4523	bytree_opt = 0.4854	Value = 0.6933
elapsed = 0.01	Round = 3	eta_opt = 0.3202	max_depth_opt = 6.0000	nrounds_opt = 88.6309	subsample_opt = 0.1219	bytree_opt = 0.4910	Value = 0.7467
elapsed = 0.01	Round = 4	eta_opt = 0.5614	max_depth_opt = 4.0000	nrounds_opt = 76.5790	subsample_opt = 0.3092	bytree_opt = 0.6768	Value = 0.9600
elapsed = 0.02	Round = 5	eta_opt = 0.3955	max_depth_opt = 6.0000	nrounds_opt = 157.8434	subsample_opt = 0.3799	bytree_opt = 0.9856	Value = 0.9867
# 中略
elapsed = 0.01	Round = 19	eta_opt = 0.6466	max_depth_opt = 5.0000	nrounds_opt = 82.4813	subsample_opt = 0.1647	bytree_opt = 0.6055	Value = 0.9733
elapsed = 0.02	Round = 20	eta_opt = 0.7080	max_depth_opt = 4.0000	nrounds_opt = 94.7421	subsample_opt = 0.7886	bytree_opt = 0.9739	Value = 0.9867
elapsed = 0.02	Round = 21	eta_opt = 0.1000	max_depth_opt = 6.0000	nrounds_opt = 93.1294	subsample_opt = 0.6490	bytree_opt = 0.9329	Value = 1.0000

Best Parameters Found:
Round = 10	eta_opt = 0.4986	max_depth_opt = 5.0000	nrounds_opt = 94.3087	subsample_opt = 0.9980	bytree_opt = 0.9219	Value = 1.0000
```

## 引数について(例: SVM)
```{r eval=FALSE}
res <- svm_opt(
  # データセット関連(最低限必要)
  train_data = iris_train,
  train_label = iris_train$Species,
  test_data = iris_test,
  test_label = iris_test$Species,
  # ここからハイパーパラメータの引数
  gamma_range = c(10 ^ (-5), 10 ^ 5),
  c_range = c(10 ^ (-2), 10 ^ 2),
  # ここからベイズ最適化の引数
  init_points = 20,
  n_iter = 1,
  acq = "ei",
  kappa = 2.576,
  eps = 0.0,
  kernel = list(type = "exponential", power = 2)
  )
```

# To Do
## まだ開発版なので...
- クロスバリデーションへの対応
- SVM のカーネル追加
- 引数の充実

# Enjoy!
